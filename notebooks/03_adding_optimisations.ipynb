{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9afe687b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b03910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_likelihood(text):\n",
    "    \"\"\"\n",
    "    Calculates the average log-likelihood of a text under a given model.\n",
    "    Lower score is better (less surprising).\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "        \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = inputs.input_ids[:, :1024].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        neg_log_likelihood = outputs.loss\n",
    "    \n",
    "    return -neg_log_likelihood.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d087b3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66ddfba7dd34bd0b4d50d55766aed98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Text 1:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e7697c17b44bf4af1e7b214151c171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Text 2:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading complete\n",
      "Train DataFrame shape: (95, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>real_text_id</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>real_text</th>\n",
       "      <th>fake_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The VIRSA (Visible Infrared Survey Telescope A...</td>\n",
       "      <td>The China relay network has released a signifi...</td>\n",
       "      <td>The VIRSA (Visible Infrared Survey Telescope A...</td>\n",
       "      <td>The China relay network has released a signifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>China\\nThe goal of this project involves achie...</td>\n",
       "      <td>The project aims to achieve an accuracy level ...</td>\n",
       "      <td>The project aims to achieve an accuracy level ...</td>\n",
       "      <td>China\\nThe goal of this project involves achie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Scientists can learn about how galaxies form a...</td>\n",
       "      <td>Dinosaur eggshells offer clues about what dino...</td>\n",
       "      <td>Scientists can learn about how galaxies form a...</td>\n",
       "      <td>Dinosaur eggshells offer clues about what dino...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>China\\nThe study suggests that multiple star s...</td>\n",
       "      <td>The importance for understanding how stars evo...</td>\n",
       "      <td>The importance for understanding how stars evo...</td>\n",
       "      <td>China\\nThe study suggests that multiple star s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Dinosaur Rex was excited about his new toy set...</td>\n",
       "      <td>Analyzing how fast stars rotate within a galax...</td>\n",
       "      <td>Analyzing how fast stars rotate within a galax...</td>\n",
       "      <td>Dinosaur Rex was excited about his new toy set...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  real_text_id                                             text_1  \\\n",
       "0   0             1  The VIRSA (Visible Infrared Survey Telescope A...   \n",
       "1   1             2  China\\nThe goal of this project involves achie...   \n",
       "2   2             1  Scientists can learn about how galaxies form a...   \n",
       "3   3             2  China\\nThe study suggests that multiple star s...   \n",
       "4   4             2  Dinosaur Rex was excited about his new toy set...   \n",
       "\n",
       "                                              text_2  \\\n",
       "0  The China relay network has released a signifi...   \n",
       "1  The project aims to achieve an accuracy level ...   \n",
       "2  Dinosaur eggshells offer clues about what dino...   \n",
       "3  The importance for understanding how stars evo...   \n",
       "4  Analyzing how fast stars rotate within a galax...   \n",
       "\n",
       "                                           real_text  \\\n",
       "0  The VIRSA (Visible Infrared Survey Telescope A...   \n",
       "1  The project aims to achieve an accuracy level ...   \n",
       "2  Scientists can learn about how galaxies form a...   \n",
       "3  The importance for understanding how stars evo...   \n",
       "4  Analyzing how fast stars rotate within a galax...   \n",
       "\n",
       "                                           fake_text  \n",
       "0  The China relay network has released a signifi...  \n",
       "1  China\\nThe goal of this project involves achie...  \n",
       "2  Dinosaur eggshells offer clues about what dino...  \n",
       "3  China\\nThe study suggests that multiple star s...  \n",
       "4  Dinosaur Rex was excited about his new toy set...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_PATH = Path('../')\n",
    "DATA_PATH = BASE_PATH / 'data'\n",
    "TRAIN_DIR = DATA_PATH / 'train'\n",
    "TEST_DIR = DATA_PATH / 'test'\n",
    "TRAIN_CSV = DATA_PATH / 'train.csv'\n",
    "\n",
    "# Load train data\n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "def get_text(file_path):\n",
    "    \"\"\"Reads text from a file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Load text content into df\n",
    "tqdm.pandas(desc=\"Loading Text 1\")\n",
    "train_df['text_1'] = train_df['id'].progress_apply(lambda x: get_text(TRAIN_DIR / f'article_{x:04d}' / 'file_1.txt'))\n",
    "\n",
    "tqdm.pandas(desc=\"Loading Text 2\")\n",
    "train_df['text_2'] = train_df['id'].progress_apply(lambda x: get_text(TRAIN_DIR / f'article_{x:04d}' / 'file_2.txt'))\n",
    "\n",
    "# Create cols for real and fake text\n",
    "train_df['real_text'] = np.where(train_df['real_text_id'] == 1, train_df['text_1'], train_df['text_2'])\n",
    "train_df['fake_text'] = np.where(train_df['real_text_id'] == 1, train_df['text_2'], train_df['text_1'])\n",
    "\n",
    "print(\"Data Loading complete\")\n",
    "print(f\"Train DataFrame shape: {train_df.shape}\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99aafd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features for text_1 and text_2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78ccd5f3fec458f952fc056d75aeffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Text 2:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbe9bdd6138445aa0e932a4d886e34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Text 2:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pairwise features...\n",
      "Feature engineering complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_len_1</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>sent_count_1</th>\n",
       "      <th>avg_word_len_1</th>\n",
       "      <th>avg_sent_len_1</th>\n",
       "      <th>stopword_ratio_1</th>\n",
       "      <th>punct_count_1</th>\n",
       "      <th>flesch_score_1</th>\n",
       "      <th>unique_word_ratio_1</th>\n",
       "      <th>char_len_2</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_sent_len_diff</th>\n",
       "      <th>avg_sent_len_ratio</th>\n",
       "      <th>stopword_ratio_diff</th>\n",
       "      <th>stopword_ratio_ratio</th>\n",
       "      <th>punct_count_diff</th>\n",
       "      <th>punct_count_ratio</th>\n",
       "      <th>flesch_score_diff</th>\n",
       "      <th>flesch_score_ratio</th>\n",
       "      <th>unique_word_ratio_diff</th>\n",
       "      <th>unique_word_ratio_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2196</td>\n",
       "      <td>304</td>\n",
       "      <td>9</td>\n",
       "      <td>6.226974</td>\n",
       "      <td>33.777778</td>\n",
       "      <td>0.194079</td>\n",
       "      <td>32</td>\n",
       "      <td>-4.937217</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>2018</td>\n",
       "      <td>...</td>\n",
       "      <td>4.177778</td>\n",
       "      <td>1.141141</td>\n",
       "      <td>-0.076191</td>\n",
       "      <td>0.718089</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>-21.858967</td>\n",
       "      <td>-0.291767</td>\n",
       "      <td>0.005334</td>\n",
       "      <td>1.006689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3124</td>\n",
       "      <td>454</td>\n",
       "      <td>9</td>\n",
       "      <td>5.883260</td>\n",
       "      <td>50.444444</td>\n",
       "      <td>0.321586</td>\n",
       "      <td>47</td>\n",
       "      <td>-3.233476</td>\n",
       "      <td>0.698238</td>\n",
       "      <td>936</td>\n",
       "      <td>...</td>\n",
       "      <td>27.611111</td>\n",
       "      <td>2.209246</td>\n",
       "      <td>0.058812</td>\n",
       "      <td>1.223808</td>\n",
       "      <td>24</td>\n",
       "      <td>2.043478</td>\n",
       "      <td>-23.044309</td>\n",
       "      <td>-0.163218</td>\n",
       "      <td>-0.155777</td>\n",
       "      <td>0.817594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139</td>\n",
       "      <td>159</td>\n",
       "      <td>4</td>\n",
       "      <td>6.169811</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>0.289308</td>\n",
       "      <td>16</td>\n",
       "      <td>0.219231</td>\n",
       "      <td>0.823899</td>\n",
       "      <td>801</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916667</td>\n",
       "      <td>0.954000</td>\n",
       "      <td>-0.030692</td>\n",
       "      <td>0.904085</td>\n",
       "      <td>7</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>-17.976952</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>-0.048101</td>\n",
       "      <td>0.944838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1774</td>\n",
       "      <td>263</td>\n",
       "      <td>8</td>\n",
       "      <td>5.749049</td>\n",
       "      <td>32.875000</td>\n",
       "      <td>0.326996</td>\n",
       "      <td>52</td>\n",
       "      <td>23.320625</td>\n",
       "      <td>0.726236</td>\n",
       "      <td>1869</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.553571</td>\n",
       "      <td>0.878340</td>\n",
       "      <td>0.056004</td>\n",
       "      <td>1.206658</td>\n",
       "      <td>35</td>\n",
       "      <td>3.058823</td>\n",
       "      <td>24.651213</td>\n",
       "      <td>-17.526566</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>1.045459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>195</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>4.764706</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>68.431667</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>871</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.416667</td>\n",
       "      <td>0.368564</td>\n",
       "      <td>0.256098</td>\n",
       "      <td>2.049992</td>\n",
       "      <td>-13</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>57.016667</td>\n",
       "      <td>5.994889</td>\n",
       "      <td>0.036824</td>\n",
       "      <td>1.043551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   char_len_1  word_count_1  sent_count_1  avg_word_len_1  avg_sent_len_1  \\\n",
       "0        2196           304             9        6.226974       33.777778   \n",
       "1        3124           454             9        5.883260       50.444444   \n",
       "2        1139           159             4        6.169811       39.750000   \n",
       "3        1774           263             8        5.749049       32.875000   \n",
       "4         195            34             3        4.764706       11.333333   \n",
       "\n",
       "   stopword_ratio_1  punct_count_1  flesch_score_1  unique_word_ratio_1  \\\n",
       "0          0.194079             32       -4.937217             0.802632   \n",
       "1          0.321586             47       -3.233476             0.698238   \n",
       "2          0.289308             16        0.219231             0.823899   \n",
       "3          0.326996             52       23.320625             0.726236   \n",
       "4          0.500000              6       68.431667             0.882353   \n",
       "\n",
       "   char_len_2  ...  avg_sent_len_diff  avg_sent_len_ratio  \\\n",
       "0        2018  ...           4.177778            1.141141   \n",
       "1         936  ...          27.611111            2.209246   \n",
       "2         801  ...          -1.916667            0.954000   \n",
       "3        1869  ...          -4.553571            0.878340   \n",
       "4         871  ...         -19.416667            0.368564   \n",
       "\n",
       "   stopword_ratio_diff  stopword_ratio_ratio  punct_count_diff  \\\n",
       "0            -0.076191              0.718089                -1   \n",
       "1             0.058812              1.223808                24   \n",
       "2            -0.030692              0.904085                 7   \n",
       "3             0.056004              1.206658                35   \n",
       "4             0.256098              2.049992               -13   \n",
       "\n",
       "   punct_count_ratio  flesch_score_diff  flesch_score_ratio  \\\n",
       "0           0.969697         -21.858967           -0.291767   \n",
       "1           2.043478         -23.044309           -0.163218   \n",
       "2           1.777778         -17.976952            0.012048   \n",
       "3           3.058823          24.651213          -17.526566   \n",
       "4           0.315789          57.016667            5.994889   \n",
       "\n",
       "   unique_word_ratio_diff  unique_word_ratio_ratio  \n",
       "0                0.005334                 1.006689  \n",
       "1               -0.155777                 0.817594  \n",
       "2               -0.048101                 0.944838  \n",
       "3                0.031579                 1.045459  \n",
       "4                0.036824                 1.043551  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textstat\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def create_features(text):\n",
    "    \"\"\"Extracts a dictionary of features from a single text\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return {\n",
    "            'char_len': 0, 'word_count': 0, 'sent_count': 0,\n",
    "            'avg_word_len': 0, 'avg_sent_len': 0, 'stopword_ratio': 0,\n",
    "            'punct_count': 0, 'flesch_score': 0, 'unique_word_ratio': 0\n",
    "        }\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    char_len = len(text)\n",
    "\n",
    "    if word_count == 0:\n",
    "        return {\n",
    "            'char_len': char_len, 'word_count': 0, 'sent_count': 0,\n",
    "            'avg_word_len': 0, 'avg_sent_len': 0, 'stopword_ratio': 0, 'punct_count': 0, 'flesch_score': 206.835, 'unique_word_ratio': 0\n",
    "        }\n",
    "    \n",
    "    sent_count = textstat.sentence_count(text)\n",
    "\n",
    "    avg_word_len = sum(len(word) for word in words) / word_count\n",
    "    avg_sent_len = word_count / sent_count if sent_count > 0 else 0\n",
    "\n",
    "    stopwords_in_text = [word for word in words if word.lower() in stop_words]\n",
    "    stopword_ratio = len(stopwords_in_text) / word_count\n",
    "\n",
    "    punct_count = len(re.findall(r'[!?,.;:\\-\\(\\)\\[\\]\"\\']', text))\n",
    "\n",
    "    unique_word_ratio = len(set(w.lower() for w in words)) / word_count\n",
    "\n",
    "    # Readability Scores\n",
    "    flesch_score = textstat.flesch_reading_ease(text)\n",
    "\n",
    "    features = {\n",
    "        'char_len': char_len,\n",
    "        'word_count': word_count,\n",
    "        'sent_count': sent_count,\n",
    "        'avg_word_len': avg_word_len,\n",
    "        'avg_sent_len': avg_sent_len,\n",
    "        'stopword_ratio': stopword_ratio,\n",
    "        'punct_count': punct_count,\n",
    "        'flesch_score': flesch_score,\n",
    "        'unique_word_ratio': unique_word_ratio,\n",
    "    }\n",
    "    return features\n",
    "\n",
    "print(\"Creating features for text_1 and text_2...\")\n",
    "\n",
    "# Apply to text_1\n",
    "feats_1 = train_df['text_1'].progress_apply(create_features)\n",
    "feats_1_df = pd.json_normalize(feats_1)\n",
    "feats_1_df.columns = [f'{col}_1' for col in feats_1_df.columns]\n",
    "\n",
    "# Apply to text_2\n",
    "feats_2 = train_df['text_2'].progress_apply(create_features)\n",
    "feats_2_df = pd.json_normalize(feats_2)\n",
    "feats_2_df.columns = [f'{col}_2' for col in feats_2_df.columns]\n",
    "\n",
    "# Concatenate features\n",
    "feature_df = pd.concat([feats_1_df, feats_2_df], axis=1)\n",
    "\n",
    "# Create Pairwise (Difference and Ratio) Features\n",
    "print(\"Creating pairwise features...\")\n",
    "base_features = [col.replace('_1', '') for col in feats_1_df.columns]\n",
    "for col in base_features:\n",
    "    epsilon = 1e-6\n",
    "    feature_df[f'{col}_diff'] = feature_df[f'{col}_1'] - feature_df[f'{col}_2']\n",
    "    feature_df[f'{col}_ratio'] = feature_df[f'{col}_1'] / (feature_df[f'{col}_2'] + epsilon)\n",
    "\n",
    "print(\"Feature engineering complete.\")\n",
    "display(feature_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc6f19b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer model...\n",
      "Encoding texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ca45dc762541d0a50f15d4c28976fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2106fd30584c42b6984a2368941ba1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created. Shape: (95, 768)\n",
      "Creating semantic features\n",
      "Semantic Features Created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sem_diff_0</th>\n",
       "      <th>sem_diff_1</th>\n",
       "      <th>sem_diff_2</th>\n",
       "      <th>sem_diff_3</th>\n",
       "      <th>sem_diff_4</th>\n",
       "      <th>sem_diff_5</th>\n",
       "      <th>sem_diff_6</th>\n",
       "      <th>sem_diff_7</th>\n",
       "      <th>sem_diff_8</th>\n",
       "      <th>sem_diff_9</th>\n",
       "      <th>...</th>\n",
       "      <th>sem_diff_759</th>\n",
       "      <th>sem_diff_760</th>\n",
       "      <th>sem_diff_761</th>\n",
       "      <th>sem_diff_762</th>\n",
       "      <th>sem_diff_763</th>\n",
       "      <th>sem_diff_764</th>\n",
       "      <th>sem_diff_765</th>\n",
       "      <th>sem_diff_766</th>\n",
       "      <th>sem_diff_767</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019083</td>\n",
       "      <td>0.006649</td>\n",
       "      <td>0.014612</td>\n",
       "      <td>-0.021872</td>\n",
       "      <td>0.013211</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>-0.006569</td>\n",
       "      <td>0.021447</td>\n",
       "      <td>0.004465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033263</td>\n",
       "      <td>0.005071</td>\n",
       "      <td>-0.003286</td>\n",
       "      <td>-0.022461</td>\n",
       "      <td>-0.016347</td>\n",
       "      <td>0.011188</td>\n",
       "      <td>0.021890</td>\n",
       "      <td>0.042394</td>\n",
       "      <td>0.019980</td>\n",
       "      <td>0.551330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002131</td>\n",
       "      <td>0.100618</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>-0.015465</td>\n",
       "      <td>-0.002975</td>\n",
       "      <td>-0.005482</td>\n",
       "      <td>0.043374</td>\n",
       "      <td>0.023411</td>\n",
       "      <td>-0.042388</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008845</td>\n",
       "      <td>-0.001277</td>\n",
       "      <td>-0.013604</td>\n",
       "      <td>0.014051</td>\n",
       "      <td>-0.015411</td>\n",
       "      <td>-0.013543</td>\n",
       "      <td>-0.029538</td>\n",
       "      <td>0.019512</td>\n",
       "      <td>-0.032364</td>\n",
       "      <td>0.610046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.073394</td>\n",
       "      <td>-0.059585</td>\n",
       "      <td>0.028701</td>\n",
       "      <td>-0.039264</td>\n",
       "      <td>0.008226</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>-0.004064</td>\n",
       "      <td>0.037426</td>\n",
       "      <td>0.115684</td>\n",
       "      <td>0.022992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032491</td>\n",
       "      <td>-0.043163</td>\n",
       "      <td>0.059642</td>\n",
       "      <td>-0.021499</td>\n",
       "      <td>-0.070247</td>\n",
       "      <td>0.023989</td>\n",
       "      <td>-0.014840</td>\n",
       "      <td>-0.031483</td>\n",
       "      <td>-0.030843</td>\n",
       "      <td>0.247873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074293</td>\n",
       "      <td>0.040511</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>-0.016028</td>\n",
       "      <td>0.043253</td>\n",
       "      <td>-0.019031</td>\n",
       "      <td>0.010677</td>\n",
       "      <td>-0.001667</td>\n",
       "      <td>-0.018004</td>\n",
       "      <td>-0.009036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051690</td>\n",
       "      <td>-0.010257</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.041261</td>\n",
       "      <td>-0.029366</td>\n",
       "      <td>-0.006718</td>\n",
       "      <td>-0.010197</td>\n",
       "      <td>-0.063887</td>\n",
       "      <td>0.570451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098058</td>\n",
       "      <td>0.052397</td>\n",
       "      <td>-0.012488</td>\n",
       "      <td>0.083774</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.008235</td>\n",
       "      <td>-0.029036</td>\n",
       "      <td>-0.039514</td>\n",
       "      <td>-0.045138</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043760</td>\n",
       "      <td>-0.012135</td>\n",
       "      <td>0.006492</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.046947</td>\n",
       "      <td>0.009172</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.035460</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>0.048204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sem_diff_0  sem_diff_1  sem_diff_2  sem_diff_3  sem_diff_4  sem_diff_5  \\\n",
       "0    0.019083    0.006649    0.014612   -0.021872    0.013211    0.006887   \n",
       "1   -0.002131    0.100618   -0.006860   -0.015465   -0.002975   -0.005482   \n",
       "2   -0.073394   -0.059585    0.028701   -0.039264    0.008226    0.002810   \n",
       "3    0.074293    0.040511   -0.004778   -0.016028    0.043253   -0.019031   \n",
       "4    0.098058    0.052397   -0.012488    0.083774    0.019192    0.008235   \n",
       "\n",
       "   sem_diff_6  sem_diff_7  sem_diff_8  sem_diff_9  ...  sem_diff_759  \\\n",
       "0    0.016760   -0.006569    0.021447    0.004465  ...      0.033263   \n",
       "1    0.043374    0.023411   -0.042388    0.009743  ...     -0.008845   \n",
       "2   -0.004064    0.037426    0.115684    0.022992  ...      0.032491   \n",
       "3    0.010677   -0.001667   -0.018004   -0.009036  ...     -0.051690   \n",
       "4   -0.029036   -0.039514   -0.045138    0.004903  ...      0.043760   \n",
       "\n",
       "   sem_diff_760  sem_diff_761  sem_diff_762  sem_diff_763  sem_diff_764  \\\n",
       "0      0.005071     -0.003286     -0.022461     -0.016347      0.011188   \n",
       "1     -0.001277     -0.013604      0.014051     -0.015411     -0.013543   \n",
       "2     -0.043163      0.059642     -0.021499     -0.070247      0.023989   \n",
       "3     -0.010257      0.004975      0.017619      0.041261     -0.029366   \n",
       "4     -0.012135      0.006492      0.002710      0.046947      0.009172   \n",
       "\n",
       "   sem_diff_765  sem_diff_766  sem_diff_767  cosine_sim  \n",
       "0      0.021890      0.042394      0.019980    0.551330  \n",
       "1     -0.029538      0.019512     -0.032364    0.610046  \n",
       "2     -0.014840     -0.031483     -0.030843    0.247873  \n",
       "3     -0.006718     -0.010197     -0.063887    0.570451  \n",
       "4      0.004975      0.035460     -0.000592    0.048204  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super feature set created. Shape: (95, 805)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Configuration\n",
    "ST_MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "print(\"Loading Sentence Transformer model...\")\n",
    "st_model = SentenceTransformer(ST_MODEL_NAME)\n",
    "\n",
    "print(\"Encoding texts...\")\n",
    "embeddings1 = st_model.encode(train_df['text_1'].tolist(), show_progress_bar=True)\n",
    "embeddings2 = st_model.encode(train_df['text_2'].tolist(), show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings created. Shape: {embeddings1.shape}\")\n",
    "\n",
    "print(\"Creating semantic features\")\n",
    "\n",
    "# 1. Cosine Similarity\n",
    "cosine_similarities = 1 - paired_cosine_distances(embeddings1, embeddings2)\n",
    "\n",
    "# 2. Element wise Differences\n",
    "embedding_diffs = embeddings1 - embeddings2\n",
    "\n",
    "# Combine two a new feature DataFrame\n",
    "semantic_features_df = pd.DataFrame(embedding_diffs)\n",
    "semantic_features_df.columns = [f'sem_diff_{i}' for i in range(embedding_diffs.shape[1])]\n",
    "semantic_features_df['cosine_sim'] = cosine_similarities\n",
    "\n",
    "print(\"Semantic Features Created\")\n",
    "display(semantic_features_df.head())\n",
    "\n",
    "# Create the super feature set\n",
    "X_super = pd.concat([feature_df, semantic_features_df], axis=1)\n",
    "y = train_df['real_text_id'] - 1\n",
    "\n",
    "print(f\"Super feature set created. Shape: {X_super.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a17ee34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating log likelihood for train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aeffc4babdc4405b54f0e09cc0df83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Log-Likelihood Text 1:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4338 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ac1bce2c234c62824de0e828b0accb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Log Likelihood Text 2:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Calculating log likelihood for train set...\")\n",
    "tqdm.pandas(desc=\"Log-Likelihood Text 1\")\n",
    "train_df['ll_score_1'] = train_df['text_1'].progress_apply(get_log_likelihood)\n",
    "\n",
    "tqdm.pandas(desc=\"Log Likelihood Text 2\")\n",
    "train_df['ll_score_2'] = train_df['text_2'].progress_apply(get_log_likelihood)\n",
    "\n",
    "# Create new feature for XGBoost model\n",
    "ll_features = pd.DataFrame()\n",
    "ll_features['ll_score_diff'] = train_df['ll_score_1'] - train_df['ll_score_2']\n",
    "ll_features['ll_score_ratio'] = train_df['ll_score_1'] / (train_df['ll_score_2'] + 1e-6)\n",
    "\n",
    "X_super_v2 = pd.concat([X_super, ll_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "854f1ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 1/5 ---\n",
      "Fold 1 Pairwise Accuracy: 0.78947\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 Pairwise Accuracy: 0.89474\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 Pairwise Accuracy: 1.00000\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 Pairwise Accuracy: 1.00000\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 Pairwise Accuracy: 0.94737\n",
      "\n",
      "Overall CV Pairwise Accuracy with SUPER Features: 0.92632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "best_xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'learning_rate': 0.04346,\n",
    "    'max_depth': 15,\n",
    "    'subsample': 0.65029,\n",
    "    'colsample_bytree': 0.8286,\n",
    "    'min_child_weight': 1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'early_stopping_rounds': 100  \n",
    "}\n",
    "\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds_super = np.zeros(len(X_super))\n",
    "models_super = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_super, y)):\n",
    "    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    X_train, y_train = X_super.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X_super.iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        **best_xgb_params\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              verbose=False)\n",
    "    \n",
    "    preds = model.predict(X_val)\n",
    "    oof_preds_super[val_idx] = preds\n",
    "    models_super.append(model)\n",
    "    \n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    print(f\"Fold {fold+1} Pairwise Accuracy: {acc:.5f}\")\n",
    "\n",
    "overall_accuracy_super = accuracy_score(y, oof_preds_super)\n",
    "print(f\"\\nOverall CV Pairwise Accuracy with SUPER Features: {overall_accuracy_super:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55223e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best blending weight: 0.48\n",
      "Ensemble CV Accuracy: 0.92632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "best_lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'random_state': 42,\n",
    "    \"learning_rate\": 0.0442,\n",
    "    \"num_leaves\": 35,\n",
    "    \"max_depth\": 8,\n",
    "    \"subsample\": 0.5160,\n",
    "    \"colsample_bytree\": 0.5952,\n",
    "    \"min_child_samples\": 28,\n",
    "    'verbose': -1  \n",
    "}\n",
    "\n",
    "oof_preds_lgbm = np.zeros(len(X_super_v2))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_super_v2, y)):\n",
    "    X_train, y_train = X_super_v2.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X_super_v2.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    lgb_model = lgb.LGBMClassifier(**best_lgb_params)\n",
    "    lgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    oof_preds_lgbm[val_idx] = lgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "\n",
    "best_xgb_params = {\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0,  \n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "oof_preds_super_proba = np.zeros(len(X_super))\n",
    "models_super = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_super, y)):\n",
    "    X_train, y_train = X_super.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X_super.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(n_estimators=1000, **best_xgb_params)\n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    oof_preds_super_proba[val_idx] = xgb_model.predict_proba(X_val)[:, 1]\n",
    "    models_super.append(xgb_model)\n",
    "\n",
    "#  Blending \n",
    "best_acc = 0\n",
    "best_weight = 0\n",
    "for w in np.arange(0, 1.01, 0.01):\n",
    "    blended_preds = (w * oof_preds_super_proba + (1-w) * oof_preds_lgbm) > 0.5\n",
    "    acc = accuracy_score(y, blended_preds)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_weight = w\n",
    "\n",
    "print(f\"Best blending weight: {best_weight:.2f}\")\n",
    "print(f\"Ensemble CV Accuracy: {best_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a8b6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6621b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/deberta-v3-small\"\n",
    "MAX_LENGTH = 512 \n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10 \n",
    "LEARNING_RATE = 1e-5 \n",
    "HEAD_LR = 1e-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449bfbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset\n",
    "class ImpostorDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.texts1 = df['text_1'].values\n",
    "        self.texts2 = df['text_2'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs1 = self.tokenizer(\n",
    "            self.texts1[idx],\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        inputs2 = self.tokenizer(\n",
    "            self.texts2[idx],\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids1': inputs1['input_ids'].squeeze(0),\n",
    "            'attention_mask1': inputs1['attention_mask'].squeeze(0),\n",
    "            'input_ids2': inputs2['input_ids'].squeeze(0),\n",
    "            'attention_mask2': inputs2['attention_mask'].squeeze(0),\n",
    "            'labels': label\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1eefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Siamese Model Architecture\n",
    "class SiameseDifferenceModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(SiameseDifferenceModel, self).__init__()\n",
    "        # The shared transformer encoder\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # We need the hidden size for the MLP input\n",
    "        hidden_size = self.transformer.config.hidden_size\n",
    "        \n",
    "        # Custom MLP head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 2) \n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        # Get embeddings for each text\n",
    "        # We take the embedding of the [CLS] token\n",
    "        out1 = self.transformer(input_ids=input_ids1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]\n",
    "        out2 = self.transformer(input_ids=input_ids2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Interaction layer\n",
    "        diff = torch.abs(out1 - out2)\n",
    "        prod = out1 * out2\n",
    "        \n",
    "        combined = torch.cat([diff, prod], dim=1)\n",
    "        \n",
    "        logits = self.head(combined)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f6ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bharg\\miniconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== FOLD 1 ===============\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cb003ba36e452587109626756d0ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb49ab1c29a4da9a025382fedecf616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b6a09ed17644d08daf5319de250940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cd047ffef14290a892f9a8deeea887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918b6c7198c34c68b4317524d4cf27f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eddc3f16e0b4d28bde751d56bea4b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbc904ef8864c7697f688086e2e401c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7da6736bb5345a1b728ce2f9dae37cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b605394400a4ca2ba3781d854528f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c0befd93c049cfa830c437834edd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val Acc: 0.4737\n",
      "\n",
      "=============== FOLD 2 ===============\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e198b05ee2448a3a98e031b59241363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00108b2bfcb748929b5e11e19d631908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709a1102676e487c81493d6fc3941ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318846fb0bcc4b69a91051583f7d4346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val Acc: 0.4211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da460ce9175439090993e87404278f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb26a39325f4437a974036b38701b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66a2664a3ea4f5088798620e9f833c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bb1cea52a340659bc301ca013dee8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b174a5a58e2f47e4add7b3c1651851fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81a1cf8b60a48d0ab847f50bacedc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val Acc: 0.4737\n",
      "\n",
      "=============== FOLD 3 ===============\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4867a6c0218b40ac8f098cf27b25cbe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f36493af494ce78cf41de0c2435b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5505bf50f2ea4e60acf1f824d8019e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44c052b4c464559bedd5a845dcfc4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d6da7f7d3f4de39b0c0f52e223bb07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cf81989c6c49ec857e9e6c77f99134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a16764135a140ef94b059c03cf35b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684d4ea41e5846eab001856845adfbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0593037e28de4643ac04622f9f9e4a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cef6bbc7287477991f9ca7fd62ba36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val Acc: 0.5263\n",
      "\n",
      "=============== FOLD 4 ===============\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5417be5db45b4271900d34f6948077ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1986336112054d6e8e76f6f1e0b4fc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d4b066f5c84ec590a098d07a530ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab1ca9417c14c9b8dbd300b04f0fcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac366df3e737465f9145ad473f6e764e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val Acc: 0.6842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6a72822f1c49258f451118b875c517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c5dcd7a7db40c3b3cbeac853ce851c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d6b71fdea6483fae46bfefdb2d0f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836fa80e7ed94aa9b846e8c59e3b4278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933be75924594321b2e7b7c8a6f66ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val Acc: 0.4737\n",
      "\n",
      "=============== FOLD 5 ===============\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3bda9148b444cc83865e3848942cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee19f10898ca4103bf907d37dd7668d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5972526d6b9f46b68e5b9944858910cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65188e9e6a2846c9953fdf1021b7dadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7983870af774ad69c56067951f94743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fd78be977d418a8925c98bb053e555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val Acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edda96f3899941658f898dc73f10ba6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val Acc: 0.6842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b02a8f8d6e45068d613b1f4e4ce420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08715e21fa164539a657b22012bfc3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val Acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8abb42ba414129acc5a1cdab109385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val Acc: 0.4737\n"
     ]
    }
   ],
   "source": [
    "# Main Training and CV Loop\n",
    "# Reload train_df and add the label column\n",
    "train_df['label'] = train_df['real_text_id'] - 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds_siamese = np.zeros(len(train_df))\n",
    "y = train_df['label'].values\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, y)):\n",
    "    print(f\"\\n{'='*15} FOLD {fold+1} {'='*15}\")\n",
    "    \n",
    "    train_data = train_df.iloc[train_idx]\n",
    "    val_data = train_df.iloc[val_idx]\n",
    "    \n",
    "    train_dataset = ImpostorDataset(train_data, tokenizer, MAX_LENGTH)\n",
    "    val_dataset = ImpostorDataset(val_data, tokenizer, MAX_LENGTH)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SiameseDifferenceModel(MODEL_NAME).to(device)\n",
    "    \n",
    "    # Differential learning rates\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": model.transformer.parameters(), \"lr\": LEARNING_RATE},\n",
    "        {\"params\": model.head.parameters(), \"lr\": HEAD_LR},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * EPOCHS\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids1 = batch['input_ids1'].to(device)\n",
    "            attention_mask1 = batch['attention_mask1'].to(device)\n",
    "            input_ids2 = batch['input_ids2'].to(device)\n",
    "            attention_mask2 = batch['attention_mask2'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids1 = batch['input_ids1'].to(device)\n",
    "                attention_mask1 = batch['attention_mask1'].to(device)\n",
    "                input_ids2 = batch['input_ids2'].to(device)\n",
    "                attention_mask2 = batch['attention_mask2'].to(device)\n",
    "                \n",
    "                logits = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                val_preds.extend(preds)\n",
    "        \n",
    "        val_acc = accuracy_score(val_data['label'], val_preds)\n",
    "        print(f\"Epoch {epoch+1} - Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            oof_preds_siamese[val_idx] = val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560ca1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Siamese Network CV Accuracy: 0.57895\n"
     ]
    }
   ],
   "source": [
    "final_cv_acc = accuracy_score(y, oof_preds_siamese)\n",
    "print(f\"\\nOverall Siamese Network CV Accuracy: {final_cv_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd3fe9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading Data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Training DataFrame is empty after filtering. Please check if file paths are correct and text files are being read.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# --- ADD THIS CHECK ---\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_df.empty:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTraining DataFrame is empty after filtering. Please check if file paths are correct and text files are being read.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# ... (rest of your code) ...\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 2: Engineering Features...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Training DataFrame is empty after filtering. Please check if file paths are correct and text files are being read."
     ]
    }
   ],
   "source": [
    "# --- 1. LOAD DATA (with added check) ---\n",
    "print(\"Step 1: Loading Data...\")\n",
    "# ... (your existing data loading code) ...\n",
    "\n",
    "# Load Train Data\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_df['text_1'] = train_df['id'].apply(lambda x: get_text(TRAIN_DIR / str(x) / 'file_1.txt'))\n",
    "train_df['text_2'] = train_df['id'].apply(lambda x: get_text(TRAIN_DIR / str(x) / 'file_2.txt'))\n",
    "train_df = train_df[(train_df['text_1'].str.len() > 0) & (train_df['text_2'].str.len() > 0)].reset_index(drop=True)\n",
    "y = train_df['real_text_id'] - 1\n",
    "\n",
    "def generate_super_features(df, st_model):\n",
    "    # Structural features\n",
    "    feats_1_df = pd.json_normalize(df['text_1'].apply(create_features))\n",
    "    feats_2_df = pd.json_normalize(df['text_2'].apply(create_features))\n",
    "    feats_1_df.columns = [f'{c}_1' for c in feats_1_df.columns]\n",
    "    feats_2_df.columns = [f'{c}_2' for c in feats_2_df.columns]\n",
    "    feature_df = pd.concat([feats_1_df, feats_2_df], axis=1)\n",
    "    base_features = [c.replace('_1', '') for c in feats_1_df.columns]\n",
    "    for col in base_features:\n",
    "        feature_df[f'{col}_diff'] = feature_df[f'{col}_1'] - feature_df[f'{col}_2']\n",
    "        feature_df[f'{col}_ratio'] = feature_df[f'{col}_1'] / (feature_df[f'{col}_2'] + 1e-6)\n",
    "        \n",
    "# --- ADD THIS CHECK ---\n",
    "if train_df.empty:\n",
    "    raise ValueError(\"Training DataFrame is empty after filtering. Please check if file paths are correct and text files are being read.\")\n",
    "\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "print(\"Step 2: Engineering Features...\")\n",
    "st_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "X_super_train = generate_super_features(train_df, st_model) # This will no longer crash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f82884",
   "metadata": {},
   "source": [
    "# Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6b35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading Data...\n",
      "Step 2: Engineering Features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c2029d5bb243658936ff3637605548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5309e390d2430ca2b36b3a095c6e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b69453c987402cb1433c374e7bf657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd124d672d14636b8f8c72e5996795e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Training Final Model and Predicting...\n",
      "Step 4: Creating Submission File...\n",
      "\n",
      "Project Complete. 'submission_v2.csv' has been generated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>real_text_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  real_text_id\n",
       "0   0             1\n",
       "1   1             2\n",
       "2   2             1\n",
       "3   3             1\n",
       "4   4             2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FINAL SUBMISSION SCRIPT\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import textstat\n",
    "import re\n",
    "\n",
    "print(\"Step 1: Loading Data...\")\n",
    "BASE_PATH = Path('../') \n",
    "DATA_PATH = BASE_PATH / 'data'\n",
    "TRAIN_DIR = DATA_PATH / 'train'\n",
    "TEST_DIR = DATA_PATH / 'test'\n",
    "TRAIN_CSV = DATA_PATH / 'train.csv'\n",
    "\n",
    "def get_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f: return f.read().strip()\n",
    "    except: return \"\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_df['text_1'] = train_df['id'].apply(lambda x: get_text(TRAIN_DIR / f'article_{x:04d}' / 'file_1.txt'))\n",
    "train_df['text_2'] = train_df['id'].apply(lambda x: get_text(TRAIN_DIR / f'article_{x:04d}' / 'file_2.txt'))\n",
    "train_df = train_df[(train_df['text_1'].str.len() > 0) & (train_df['text_2'].str.len() > 0)].reset_index(drop=True)\n",
    "y = train_df['real_text_id'] - 1\n",
    "\n",
    "test_dirs = [d for d in TEST_DIR.iterdir() if d.is_dir()]\n",
    "test_ids = sorted([int(d.name.split('_')[1]) for d in test_dirs])\n",
    "\n",
    "test_df = pd.DataFrame({'id': test_ids})\n",
    "test_df['text_1'] = test_df['id'].apply(lambda x: get_text(TEST_DIR / f'article_{x:04d}' / 'file_1.txt'))\n",
    "test_df['text_2'] = test_df['id'].apply(lambda x: get_text(TEST_DIR / f'article_{x:04d}' / 'file_2.txt'))\n",
    "\n",
    "print(\"Step 2: Engineering Features...\")\n",
    "\n",
    "def create_features(text):\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return {'char_len': 0, 'word_count': 0, 'sent_count': 0, 'avg_word_len': 0, 'avg_sent_len': 0, 'flesch_score': 0}\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    if word_count == 0:\n",
    "        return {'char_len': len(text), 'word_count': 0, 'sent_count': 0, 'avg_word_len': 0, 'avg_sent_len': 0, 'flesch_score': 206.835}\n",
    "    sent_count = textstat.sentence_count(text)\n",
    "    return {\n",
    "        'char_len': len(text), 'word_count': word_count, 'sent_count': sent_count,\n",
    "        'avg_word_len': sum(len(w) for w in words) / word_count,\n",
    "        'avg_sent_len': word_count / sent_count if sent_count > 0 else 0,\n",
    "        'flesch_score': textstat.flesch_reading_ease(text),\n",
    "    }\n",
    "\n",
    "def generate_super_features(df, st_model):\n",
    "    # Structural features\n",
    "    feats_1_df = pd.json_normalize(df['text_1'].apply(create_features))\n",
    "    feats_2_df = pd.json_normalize(df['text_2'].apply(create_features))\n",
    "    feats_1_df.columns = [f'{c}_1' for c in feats_1_df.columns]\n",
    "    feats_2_df.columns = [f'{c}_2' for c in feats_2_df.columns]\n",
    "    feature_df = pd.concat([feats_1_df, feats_2_df], axis=1)\n",
    "    base_features = [c.replace('_1', '') for c in feats_1_df.columns]\n",
    "    for col in base_features:\n",
    "        feature_df[f'{col}_diff'] = feature_df[f'{col}_1'] - feature_df[f'{col}_2']\n",
    "        feature_df[f'{col}_ratio'] = feature_df[f'{col}_1'] / (feature_df[f'{col}_2'] + 1e-6)\n",
    "    \n",
    "    # Semantic features\n",
    "    embeddings1 = st_model.encode(df['text_1'].tolist(), show_progress_bar=True)\n",
    "    embeddings2 = st_model.encode(df['text_2'].tolist(), show_progress_bar=True)\n",
    "    cosine_similarities = 1 - paired_cosine_distances(embeddings1, embeddings2)\n",
    "    embedding_diffs = embeddings1 - embeddings2\n",
    "    semantic_features_df = pd.DataFrame(embedding_diffs, columns=[f'sem_diff_{i}' for i in range(embedding_diffs.shape[1])])\n",
    "    semantic_features_df['cosine_sim'] = cosine_similarities\n",
    "    \n",
    "    return pd.concat([feature_df, semantic_features_df], axis=1)\n",
    "\n",
    "st_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "X_super_train = generate_super_features(train_df, st_model)\n",
    "X_super_test = generate_super_features(test_df, st_model)\n",
    "X_super_test = X_super_test[X_super_train.columns] # Ensure column order matches\n",
    "\n",
    "print(\"Step 3: Training Final Model and Predicting...\")\n",
    "best_xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'learning_rate': 0.04346,\n",
    "    'max_depth': 15,\n",
    "    'subsample': 0.65029,\n",
    "    'colsample_bytree': 0.8286,\n",
    "    'min_child_weight': 1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "final_model = xgb.XGBClassifier(**best_xgb_params)\n",
    "final_model.fit(X_super_train, y)\n",
    "test_predictions = final_model.predict(X_super_test)\n",
    "\n",
    "# CREATE SUBMISSION FILE\n",
    "print(\"Step 4: Creating Submission File...\")\n",
    "submission_df = pd.DataFrame({'id': test_df['id']})\n",
    "submission_df['real_text_id'] = test_predictions + 1\n",
    "submission_df.to_csv('submission_v2.csv', index=False)\n",
    "\n",
    "print(\"\\nProject Complete. 'submission_v2.csv' has been generated.\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fc522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
